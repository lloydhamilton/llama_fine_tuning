{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:43:19.412298Z",
     "start_time": "2025-05-08T19:43:11.830695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer, QuantoConfig\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"squad\", split=\"train\")\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "base_model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "quant_config = QuantoConfig(weights=\"int8\")\n",
    "\n",
    "def fetch_model(is_quant: bool=False):\n",
    "    config = dict(\n",
    "        pretrained_model_name_or_path=base_model_path,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        # torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quant_config,\n",
    "    )\n",
    "    if is_quant:\n",
    "        quant_cfg = {\n",
    "            \"quantization_config\": quant_config,\n",
    "        }\n",
    "        config = config | quant_cfg\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(**config)\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    return model\n",
    "\n",
    "def get_trainable_params(model) -> None:\n",
    "    \"\"\"\n",
    "    Get the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {params}\")\n",
    "\n",
    "quant_model = fetch_model(is_quant=True)\n",
    "base_model = fetch_model(is_quant=False)"
   ],
   "id": "2bfbd7d022ab5f94",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 2\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_model.config._name_or_path",
   "id": "7be58e399212e5fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "id": "24f7732435f186c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_trainable_params(base_model)",
   "id": "4d78216925069d8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_trainable_params(quant_model)",
   "id": "44bbea068639871f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\n",
    "        \"o_proj\", \"gate_proj\", \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "lora_base_model = prepare_model_for_kbit_training(base_model)\n",
    "lora_base_model = get_peft_model(lora_base_model, lora_config)"
   ],
   "id": "6093d2dc889c18b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_trainable_params(lora_base_model)",
   "id": "b4bb9a9b8c6febb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_trainable_params(quant_model)",
   "id": "b4a0126320ec4ab6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "11272192/262735872",
   "id": "490c3aedb402a5eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Who is Vincent van Gogh?\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "1c6e90152020f2d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a skilled Python developer specializing in database management and optimization.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm experiencing a sorting issue in my database. Could you please provide Python code to help resolve this problem?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=512, do_sample=True)\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        outputs[0][\"generated_text\"].split(\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        )[1]\n",
    "    )\n",
    ")"
   ],
   "id": "b58e988a29eacb92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data[\"train\"][0]",
   "id": "347ea2e4fcc2b639",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def apply_chat_template(example):\n",
    "    answer = example['answers']['text']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "templated_dataset = data.map(apply_chat_template)"
   ],
   "id": "9b4588810dcdccb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "        input_dataset: dict, tokeniser: PreTrainedTokenizerFast\n",
    ") -> dict:\n",
    "    \"\"\"Preprocess the input dataset to fit expected format.\n",
    "\n",
    "    See: https://huggingface.co/docs/trl/en/sft_trainer for details.\n",
    "    \"\"\"\n",
    "    answer = input_dataset['answers']['text']\n",
    "    message_template = [\n",
    "        {\"role\": \"system\", \"content\": input_dataset['context']},\n",
    "        {\"role\": \"user\", \"content\": input_dataset['question'][0]},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    # messages = tokeniser.apply_chat_template(\n",
    "    #     message_template, tokenize=False, add_generation_prompt=True\n",
    "    # )\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def apply_message_template(\n",
    "    input_dataset: dict\n",
    ") -> dict:\n",
    "    \"\"\"Preprocess the input dataset to fit expected format.\n",
    "\n",
    "    See: https://huggingface.co/docs/trl/en/sft_trainer for details.\n",
    "    \"\"\"\n",
    "    answer = input_dataset['answers']['text']\n",
    "    if isinstance(input_dataset['answers']['text'], list):\n",
    "        answer = input_dataset['answers']['text'][0]\n",
    "    message_template = [\n",
    "        {\"role\": \"system\", \"content\": input_dataset['context']},\n",
    "        {\"role\": \"user\", \"content\": input_dataset['question']},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    return {\"messages\": message_template}\n",
    "\n",
    "templated_dataset = data.map(apply_message_template)"
   ],
   "id": "3b33a32a01a8fa0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "templated_dataset[0]",
   "id": "f7c648bcf8662811",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "db8ae49fb6740633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tokenise the prompts\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(\n",
    "        example['prompt'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    # Set padding token labels to -100 to ignore them in loss calculation\n",
    "    tokens['labels'] = [\n",
    "        -100 if token == tokenizer.pad_token_id else token for token in tokens['input_ids']\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "tokenised_dataset = templated_dataset.map(tokenize_function)\n",
    "tokenised_dataset = tokenised_dataset.remove_columns([\"title\", \"context\", \"question\", \"answers\", \"prompt\"])"
   ],
   "id": "d16dcc2e5d39ea62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
    "# # https://huggingface.co/docs/diffusers/en/quantization/quanto\n",
    "# for name, module in model.named_modules(): # https://www.datacamp.com/tutorial/fine-tuning-llama-3-2\n",
    "#     print(f\"name: {name}\")\n",
    "#     print(f\"module: {module}\")\n"
   ],
   "id": "90175f498ffc230f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "lora_base_model.train()\n",
    "training_args = SFTConfig(\n",
    "    output_dir = \"cp\",\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    eval_accumulation_steps = 8,\n",
    "    # optim = \"paged_adamw_32bit\",\n",
    "    save_steps = 10,\n",
    "    logging_steps = 10,\n",
    "    learning_rate = 5e-5,\n",
    "    max_grad_norm = 0.3,\n",
    "    max_steps = 50,\n",
    "    warmup_ratio = 0.03,\n",
    "    eval_strategy=\"steps\",\n",
    "    lr_scheduler_type = \"linear\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=lora_base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=lora_base_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenised_dataset[\"train\"],\n",
    "#     eval_dataset=tokenised_dataset[\"validation\"],\n",
    "#     tokenizer=tokenizer)\n",
    "#\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "#\n",
    "# # Save the model and tokenizer\n",
    "# trainer.save_model(\"./fine-tuned-model\")\n",
    "# tokenizer.save_pretrained(\"./fine-tuned-model\")"
   ],
   "id": "c5e8e4cc98d8b052"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "path = \"raw_data/ConvFinQA/train.json\"\n",
    "with open(path, \"r\") as file:\n",
    "    data = json.load(file)"
   ],
   "id": "ef48d34bb81dcb54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data[0]",
   "id": "5a755aafa8aa9331"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "def load_lora_model(base_model_path, adapter_path):\n",
    "    # First load the base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Then load the LoRA adapter weights\n",
    "    fine_tuned_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    return fine_tuned_model, base_model, tokenizer\n",
    "model_path = \"../cp/checkpoint-60\"  # Your output directory from training\n",
    "base_model_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "ft_model, base_model, tokeniser = load_lora_model(base_model_path, model_path)"
   ],
   "id": "de37e09ac2478192",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "val_data = load_dataset(\"squad\")\n",
    "\n",
    "ft_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ft_model,\n",
    "    tokenizer=tokeniser,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "base_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokeniser,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "id": "eaa0084b0799db7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "val_data[\"train\"][1]",
   "id": "3fc17ff10cc117a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:07:06.979841Z",
     "start_time": "2025-05-08T21:07:06.942717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is in front of the Notre Dame Main Building?\"}]\n",
    "\n",
    "prompt = tokeniser.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = base_pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "ebc3215a12ce5582",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokeniser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[55]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m messages = [{\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mWhat is in front of the Notre Dame Main Building?\u001B[39m\u001B[33m\"\u001B[39m}]\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m prompt = \u001B[43mtokeniser\u001B[49m.apply_chat_template(\n\u001B[32m      4\u001B[39m     messages, tokenize=\u001B[38;5;28;01mFalse\u001B[39;00m, add_generation_prompt=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m      5\u001B[39m )\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m      7\u001B[39m     outputs = base_pipe(prompt, max_new_tokens=\u001B[32m120\u001B[39m, do_sample=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mNameError\u001B[39m: name 'tokeniser' is not defined"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with torch.no_grad():\n",
    "    outputs = ft_pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "f61f19aafffc88df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:08:34.067038Z",
     "start_time": "2025-05-08T21:08:29.905035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from train.fine_tune import CustomFineTuner\n",
    "from transformers import QuantoConfig\n",
    "from peft import LoraConfig\n",
    "HUGGING_FACE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "q_cfg = QuantoConfig(weights=\"int8\")\n",
    "fine_tuner = CustomFineTuner(\n",
    "    huggingface_model=HUGGING_FACE_MODEL,\n",
    "    lora_config=lora_cfg,\n",
    ")\n",
    "\n",
    "model = fine_tuner.fetch_model()"
   ],
   "id": "b08c5c1299dc4e24",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-05-08 22:08:34.026\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mtrain.fine_tune\u001B[0m:\u001B[36mlog_trainable_params\u001B[0m:\u001B[36m98\u001B[0m - \u001B[1mTotal trainable parameters for meta-llama/Llama-3.2-1B-Instruct: 1235814400\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:08:35.068147Z",
     "start_time": "2025-05-08T21:08:34.114571Z"
    }
   },
   "cell_type": "code",
   "source": "fine_tuner.generate_predictions(model, input=\"hello\")",
   "id": "1ec0ab262e1903b2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:13:12.707767Z",
     "start_time": "2025-05-08T21:13:09.552067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "tokenizer_no_pad = AutoTokenizer.from_pretrained(\n",
    "    HUGGING_FACE_MODEL,\n",
    "    add_bos_token=True\n",
    ")\n",
    "\n",
    "base_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer_no_pad,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "\n",
    "prompt = tokenizer_no_pad.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = base_pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "9e2beb7102e8a397",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 08 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hello. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:11:39.492140Z",
     "start_time": "2025-05-08T21:10:55.765488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from transformers import pipeline\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "mlflow.set_experiment(\"Llama-3.2-1B-Instruct\")\n",
    "with mlflow.start_run():\n",
    "    tokenizer_no_pad = AutoTokenizer.from_pretrained(\n",
    "        HUGGING_FACE_MODEL,\n",
    "        add_bos_token=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        HUGGING_FACE_MODEL,\n",
    "        device_map=\"auto\",\n",
    "\n",
    "    )\n",
    "    base_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer_no_pad,\n",
    "        # torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # Ensure pad_token_id is an integer before logging\n",
    "    # if isinstance(model.generation_config.pad_token_id, list):\n",
    "    #     model.generation_config.pad_token_id = model.generation_config.pad_token_id[0]\n",
    "    logged_model = mlflow.transformers.log_model(\n",
    "        transformers_model=base_pipe,\n",
    "        task=\"text-generation\",\n",
    "        artifact_path=\"model\",\n",
    "        signature=infer_signature(\n",
    "            model_input={\n",
    "                \"content\": \"What is in front of the Notre Dame Main Building?\",\n",
    "            },\n",
    "        ),\n",
    "    )"
   ],
   "id": "e8bef1220c488c8e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "2025/05/08 22:11:22 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run rare-grouse-884 at: http://127.0.0.1:8080/#/experiments/821275711990790082/runs/4f907bbfd18b4475ab15b8a37870b1d6\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/821275711990790082\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:13:49.040059Z",
     "start_time": "2025-05-08T21:13:23.543263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loaded_model = mlflow.transformers.load_model(\n",
    "    logged_model.model_uri,\n",
    ")"
   ],
   "id": "e8f6cbab3b01a20",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 20/22 [00:15<00:02,  1.01s/it] WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: 127.0.0.1. Connection pool size: 10\n",
      "Downloading artifacts:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 21/22 [00:18<00:01,  1.30s/it]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: 127.0.0.1. Connection pool size: 10\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:18<00:00,  1.20it/s]\n",
      "2025/05/08 22:13:41 INFO mlflow.transformers: 'runs:/4f907bbfd18b4475ab15b8a37870b1d6/model' resolved as 'mlflow-artifacts:/821275711990790082/4f907bbfd18b4475ab15b8a37870b1d6/artifacts/model'\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 173.22it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 68.23it/s]\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:14:02.246554Z",
     "start_time": "2025-05-08T21:14:01.306090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "\n",
    "prompt = tokenizer_no_pad.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(prompt, max_new_tokens=120, do_sample=True)\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "fd96203bed7d4794",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 08 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:14:09.528267Z",
     "start_time": "2025-05-08T21:14:08.341015Z"
    }
   },
   "cell_type": "code",
   "source": "loaded_model(\"hello\", do_sample=False)",
   "id": "3adf09ccc4bd753d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lloydhamilton/Documents/PersonalProjects.nosync/tomorro_llm_tech/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/lloydhamilton/Documents/PersonalProjects.nosync/tomorro_llm_tech/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'hello, how can i get a free gift card to a popular restaurant chain?\\nHere are some ways to'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T21:24:24.445268Z",
     "start_time": "2025-05-08T21:24:23.478505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "\n",
    "prompt = loaded_model.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(prompt, max_new_tokens=120, do_sample=True)\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "bf9b82ccf2956b66",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 08 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "de7961d59e0ded0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
