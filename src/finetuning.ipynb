{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T19:43:19.412298Z",
     "start_time": "2025-05-08T19:43:11.830695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer, QuantoConfig\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"squad\", split=\"train\")\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "base_model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "quant_config = QuantoConfig(weights=\"int8\")\n",
    "\n",
    "def fetch_model(is_quant: bool=False):\n",
    "    config = dict(\n",
    "        pretrained_model_name_or_path=base_model_path,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        # torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quant_config,\n",
    "    )\n",
    "    if is_quant:\n",
    "        quant_cfg = {\n",
    "            \"quantization_config\": quant_config,\n",
    "        }\n",
    "        config = config | quant_cfg\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(**config)\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    return model\n",
    "\n",
    "def get_trainable_params(model) -> None:\n",
    "    \"\"\"\n",
    "    Get the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {params}\")\n",
    "\n",
    "quant_model = fetch_model(is_quant=True)\n",
    "base_model = fetch_model(is_quant=False)"
   ],
   "id": "2bfbd7d022ab5f94",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 2\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_model.config._name_or_path",
   "id": "7be58e399212e5fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "id": "24f7732435f186c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_trainable_params(base_model)",
   "id": "4d78216925069d8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_trainable_params(quant_model)",
   "id": "44bbea068639871f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\n",
    "        \"o_proj\", \"gate_proj\", \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "lora_base_model = prepare_model_for_kbit_training(base_model)\n",
    "lora_base_model = get_peft_model(lora_base_model, lora_config)"
   ],
   "id": "6093d2dc889c18b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_trainable_params(lora_base_model)",
   "id": "b4bb9a9b8c6febb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_trainable_params(quant_model)",
   "id": "b4a0126320ec4ab6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "11272192/262735872",
   "id": "490c3aedb402a5eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Who is Vincent van Gogh?\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "1c6e90152020f2d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a skilled Python developer specializing in database management and optimization.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm experiencing a sorting issue in my database. Could you please provide Python code to help resolve this problem?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=512, do_sample=True)\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        outputs[0][\"generated_text\"].split(\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        )[1]\n",
    "    )\n",
    ")"
   ],
   "id": "b58e988a29eacb92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data[\"train\"][0]",
   "id": "347ea2e4fcc2b639",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def apply_chat_template(example):\n",
    "    answer = example['answers']['text']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "templated_dataset = data.map(apply_chat_template)"
   ],
   "id": "9b4588810dcdccb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "        input_dataset: dict, tokeniser: PreTrainedTokenizerFast\n",
    ") -> dict:\n",
    "    \"\"\"Preprocess the input dataset to fit expected format.\n",
    "\n",
    "    See: https://huggingface.co/docs/trl/en/sft_trainer for details.\n",
    "    \"\"\"\n",
    "    answer = input_dataset['answers']['text']\n",
    "    message_template = [\n",
    "        {\"role\": \"system\", \"content\": input_dataset['context']},\n",
    "        {\"role\": \"user\", \"content\": input_dataset['question'][0]},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    # messages = tokeniser.apply_chat_template(\n",
    "    #     message_template, tokenize=False, add_generation_prompt=True\n",
    "    # )\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def apply_message_template(\n",
    "    input_dataset: dict\n",
    ") -> dict:\n",
    "    \"\"\"Preprocess the input dataset to fit expected format.\n",
    "\n",
    "    See: https://huggingface.co/docs/trl/en/sft_trainer for details.\n",
    "    \"\"\"\n",
    "    answer = input_dataset['answers']['text']\n",
    "    if isinstance(input_dataset['answers']['text'], list):\n",
    "        answer = input_dataset['answers']['text'][0]\n",
    "    message_template = [\n",
    "        {\"role\": \"system\", \"content\": input_dataset['context']},\n",
    "        {\"role\": \"user\", \"content\": input_dataset['question']},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    return {\"messages\": message_template}\n",
    "\n",
    "templated_dataset = data.map(apply_message_template)"
   ],
   "id": "3b33a32a01a8fa0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "templated_dataset[0]",
   "id": "f7c648bcf8662811",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "db8ae49fb6740633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tokenise the prompts\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(\n",
    "        example['prompt'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    # Set padding token labels to -100 to ignore them in loss calculation\n",
    "    tokens['labels'] = [\n",
    "        -100 if token == tokenizer.pad_token_id else token for token in tokens['input_ids']\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "tokenised_dataset = templated_dataset.map(tokenize_function)\n",
    "tokenised_dataset = tokenised_dataset.remove_columns([\"title\", \"context\", \"question\", \"answers\", \"prompt\"])"
   ],
   "id": "d16dcc2e5d39ea62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora\n",
    "# # https://huggingface.co/docs/diffusers/en/quantization/quanto\n",
    "# for name, module in model.named_modules(): # https://www.datacamp.com/tutorial/fine-tuning-llama-3-2\n",
    "#     print(f\"name: {name}\")\n",
    "#     print(f\"module: {module}\")\n"
   ],
   "id": "90175f498ffc230f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "lora_base_model.train()\n",
    "training_args = SFTConfig(\n",
    "    output_dir = \"cp\",\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    eval_accumulation_steps = 8,\n",
    "    # optim = \"paged_adamw_32bit\",\n",
    "    save_steps = 10,\n",
    "    logging_steps = 10,\n",
    "    learning_rate = 5e-5,\n",
    "    max_grad_norm = 0.3,\n",
    "    max_steps = 50,\n",
    "    warmup_ratio = 0.03,\n",
    "    eval_strategy=\"steps\",\n",
    "    lr_scheduler_type = \"linear\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=lora_base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=lora_base_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenised_dataset[\"train\"],\n",
    "#     eval_dataset=tokenised_dataset[\"validation\"],\n",
    "#     tokenizer=tokenizer)\n",
    "#\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "#\n",
    "# # Save the model and tokenizer\n",
    "# trainer.save_model(\"./fine-tuned-model\")\n",
    "# tokenizer.save_pretrained(\"./fine-tuned-model\")"
   ],
   "id": "c5e8e4cc98d8b052"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "path = \"raw_data/ConvFinQA/train.json\"\n",
    "with open(path, \"r\") as file:\n",
    "    data = json.load(file)"
   ],
   "id": "ef48d34bb81dcb54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data[0]",
   "id": "5a755aafa8aa9331"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "def load_lora_model(base_model_path, adapter_path):\n",
    "    # First load the base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Then load the LoRA adapter weights\n",
    "    fine_tuned_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    return fine_tuned_model, base_model, tokenizer\n",
    "model_path = \"../cp/checkpoint-60\"  # Your output directory from training\n",
    "base_model_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "ft_model, base_model, tokeniser = load_lora_model(base_model_path, model_path)"
   ],
   "id": "de37e09ac2478192"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "val_data = load_dataset(\"squad\")\n",
    "\n",
    "ft_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ft_model,\n",
    "    tokenizer=tokeniser,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "base_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokeniser,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "id": "eaa0084b0799db7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "val_data[\"train\"][1]",
   "id": "3fc17ff10cc117a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is in front of the Notre Dame Main Building?\"}]\n",
    "\n",
    "prompt = tokeniser.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = base_pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "ebc3215a12ce5582"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with torch.no_grad():\n",
    "    outputs = ft_pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "f61f19aafffc88df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T20:03:20.655642Z",
     "start_time": "2025-05-08T20:03:15.260268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from train.fine_tune import CustomFineTuner\n",
    "from transformers import QuantoConfig\n",
    "from peft import LoraConfig\n",
    "HUGGING_FACE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "fine_tuner = CustomFineTuner(\n",
    "    huggingface_model=HUGGING_FACE_MODEL,\n",
    "    lora_config=lora_cfg,\n",
    ")\n",
    "q_cfg = QuantoConfig(weights=\"int8\")\n",
    "\n",
    "model = fine_tuner.fetch_model()\n"
   ],
   "id": "b08c5c1299dc4e24",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lloydhamilton/Documents/PersonalProjects.nosync/tomorro_llm_tech/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "python-dotenv could not parse statement starting at line 2\n",
      "\u001B[32m2025-05-08 21:03:20.654\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mtrain.fine_tune\u001B[0m:\u001B[36mlog_trainable_params\u001B[0m:\u001B[36m98\u001B[0m - \u001B[1mTotal trainable parameters for meta-llama/Llama-3.2-1B-Instruct: 1235814400\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T20:03:22.824183Z",
     "start_time": "2025-05-08T20:03:21.755116Z"
    }
   },
   "cell_type": "code",
   "source": "fine_tuner.generate_predictions(model, input=\"hello\")",
   "id": "1ec0ab262e1903b2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello. How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-08T20:03:23.795697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"Llama-3.2-1B-Instruct\")\n",
    "with mlflow.start_run():\n",
    "    logged_model = mlflow.transformers.log_model(\n",
    "        transformers_model=dict(model=model, tokenizer=tokeniser),\n",
    "        artifact_path=\"model\",\n",
    "        signature=infer_signature(\n",
    "            model_input={\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is in front of the Notre Dame Main Building?\",\n",
    "            },\n",
    "        ),\n",
    "    )"
   ],
   "id": "e8bef1220c488c8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e8f6cbab3b01a20"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
